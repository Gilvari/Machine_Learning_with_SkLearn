# -*- coding: utf-8 -*-
"""LinearRegression_IRIS_DATASET.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MP_XxeoCGobUyjtYqNGeXem2G_zowLFp
"""

# IRIS dataset: The matrix comprises 150 rows and 4 columns, representing samples and four distinct measurements: Sepal Length, Sepal Width, Petal Length, and Petal Width, respectively.

import sklearn as sk
import pandas as pd
from sklearn import datasets
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

data = datasets.load_iris()
df = pd.DataFrame(data.data,columns=data.feature_names) #creating new data frame with iris samples' values
df['species']=data.target #add new column as species which gain the value from the target value of iris dataset
df.head()

feature_names=['sepal length (cm)','sepal width (cm)','petal length (cm)','petal width (cm)'] #arrange data into features and target!
df.loc[:,feature_names]
print(df.loc[:,feature_names])
x= df.loc[:,feature_names].values #changing to numpy array
print(x.shape)

df.loc[:,'species'] #always target factoris one dimensional
print(df.loc[:,'species'])
y= df.loc[:,'species'].values #changing to numpy array
print(y.shape)

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

df.isnull().sum() #if there were any null or dismiss value you can use drop

reg=LinearRegression(fit_intercept=True) #like when we create an object from the class

reg.fit(x,y) #model is learning the relationship between x and y

reg.predict(x)

score=reg.score(x,y) # how to measure the performance of the model! :) easy!
print(score)

reg.coef_
reg.intercept_
n=reg.coef_[0]
b=reg.intercept_
print("Coefficient (n): {}, Intercept (b): {}".format(n, b))
print("Equation: y = {:.2f}x + {:.2f}".format(n, b)) #just finding the formula with round values of n and b

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# the x has 4 different features and the target variable y is one-dimensional so for comparing each features compatibility with the target domain we need below settings
# honestly this dataset is not a good example for showing the regressionline, I used anyway :D
fig, ax = plt.subplots(nrows=1, ncols=5, figsize=(15, 4)) #here we have different subplots for each features comparison to target

for i in range(4):
    ax[i].scatter(x[:, i], y, color='black')
    feature_values = x[:, i]
    predictions = reg.predict(x)  # Predictions for all data points
    ax[i].scatter(feature_values, predictions, color='red', marker='x', s=20)

    ax[i].set_xlabel(f'Feature {i+1}')
    ax[i].set_ylabel('Target')
    ax[i].set_title(f'Feature {i+1} vs. Target')

ax[4].hist(y, bins=20, edgecolor='black')
ax[4].set_xlabel('Target')
ax[4].set_ylabel('Frequency')
ax[4].set_title('Target Variable Distribution')

plt.tight_layout() # preventing overlap
plt.show()